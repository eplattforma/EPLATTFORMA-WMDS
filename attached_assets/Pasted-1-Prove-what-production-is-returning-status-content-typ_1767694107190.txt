1) Prove what production is returning (status + content-type)

Run this against production:

curl -i -X POST "https://<prod-host>/api/powersoft/sync/invoices?date=2025-12-28"


You want to see:

HTTP status (200 vs 500 vs 502/504)

Content-Type (application/json vs text/plain/html)

If it’s 502/504 or text/html/text/plain, the response is being generated by the platform/proxy, not by your Flask jsonify.

2) Check production logs for “killed / timeout / worker timeout”

In Replit Deployments logs, look specifically for:

“WORKER TIMEOUT”

“Killed”

“OOM”

stack traces that stop mid-import

If the process is killed or timed out, Flask will not return your JSON error response.

3) Confirm production is running with a real WSGI server and sane timeouts

Development often runs a permissive dev server; production often runs Gunicorn (or similar) with stricter defaults.

If you use Gunicorn in production, set timeouts explicitly (example):

gunicorn main:app --workers 2 --threads 4 --timeout 180 --graceful-timeout 180


Key checks:

If production has 1 worker, a long import can block it and lead to gateway errors.

If timeout is 30/60 seconds, bulk imports will intermittently fail.

4) Add timeouts to every outbound call (PS365, shelves, barcode)

A single “hung” outbound call will stall the request until the platform times it out.

In call_ps365 / fetch_item_shelves / find_barcode_for_item_ps365, ensure you use request timeouts like:

connect timeout + read timeout (example: timeout=(5, 30)), not an infinite wait.

Also add simple timing logs per call (start/end and elapsed seconds). In production, this very quickly identifies the slow/hung dependency.

5) Check production database lock contention (this is production-only even with same import volume)

Production has live users updating the same invoice_items rows (picking, locking, resetting). That can cause your import UPDATEs to wait on row locks.

This can make the same import:

fast in dev (no concurrent writers)

slow/hang in prod (row lock waits) → gateway “Internal Server Error”

What to check on Neon during the failure:

long-running queries

lock waits

Mitigations:

If an item is locked_by_batch_id != NULL, consider skipping all updates for that row during import (any UPDATE will take a row lock, even if you “preserve” fields).

Keep transactions short (you already commit per invoice, which is good).

Optional defensive setting (per session) to avoid indefinite lock waits:

lock_timeout / statement_timeout (so you fail fast with a controlled JSON error rather than hanging until the proxy kills you).

6) Check SQLAlchemy connection pool settings (prod-only bottleneck)

Production often has higher concurrency, and the default pool settings can cause requests to block waiting for a DB connection.

In your Flask config, set explicit engine options (example):

SQLALCHEMY_ENGINE_OPTIONS = {
    "pool_pre_ping": True,
    "pool_size": 5,
    "max_overflow": 10,
    "pool_timeout": 30,
}


Symptoms of pool exhaustion: requests hang, then proxy returns non-JSON error.

7) Make the client resilient to non-JSON errors (so you can see the real body)

Even after fixing, this helps diagnosis and prevents misleading “Unexpected token”:

read response.text() first, then JSON.parse only if it’s JSON

display status code + first 200–300 chars of body on error

8) Practical production-grade approach (recommended)

Bulk imports should not run as a single synchronous HTTP request. In production, do one of these:

process one page per request and return next_page

push the job to a background worker/queue and return a “job id”

This eliminates timeout/gateway issues entirely.