Below are the most critical recommendations for your Replit/Flask warehouse system, prioritized by impact on production stability, data safety, and operability.

P0 — Must address (stability + security)
1) Remove production-only DB timeouts from “heavy” operations (or scope them)

In production you explicitly set PostgreSQL statement_timeout=60000 and lock_timeout=10000. That is a classic reason why “import by date” fails in production while “single invoice” works (the date import is heavier and more likely to exceed those limits). 
GitHub

Recommendation

Make these timeouts configurable per environment, and/or

Override them only for the invoice-sync session (e.g., set higher timeouts inside the sync job), and/or

Split the sync so no single statement/transaction runs long enough to trip the timeout.

This is the single highest-probability “prod-only” difference to fix. 
GitHub

2) Convert “import by date” into a background job (do not run it as a single HTTP request)

Even if you raise timeouts, long-running sync inside an HTTP request is fragile (gunicorn/request timeouts, worker restarts, transient network, etc.). In your app you already initialize a scheduler; leverage that for “sync jobs.” 
GitHub
+1

Recommendation

/api/powersoft/sync/invoices?date=... should enqueue a job and immediately return {job_id}.

UI polls /api/jobs/<job_id> for progress and final results.

Store job status in DB (started_at, finished_at, success, error, counts).

This also gives you auditability and safe retries.

3) Enforce a strict JSON error contract for all /api/* endpoints

Your error symptom (“Unexpected token 'I' … not valid JSON”) typically means the frontend attempted response.json() but received plain text/HTML (often a generic “Internal Server Error”). Your routes_powersoft correctly returns JSON in normal error paths, but you still need a global JSON error handler so unexpected exceptions never produce HTML. 
GitHub

Recommendation

Add a Flask @app.errorhandler(Exception) that returns JSON when request.path.startswith("/api/").

In the frontend, only call response.json() if the response content-type includes application/json; otherwise read .text() and display it.

This prevents “mystery JSON parse errors” and surfaces the real backend stack trace / message.

4) Remove credential artifacts from Git immediately (cookies.txt)

You have cookies.txt tracked in git. That is frequently a sensitive session artifact and should not be in source control.

Recommendation

Remove it from tracking, add to .gitignore, and if the repo was ever public/shared, treat it as compromised and rotate anything it might authenticate.

(You already identified it with git ls-files | grep cookies.txt.)

P1 — High priority (data correctness under real warehouse concurrency)
5) Make the sync “lock-aware” and “picker-safe”

Production has real concurrency: pickers are updating invoice_items while a date import is trying to update the same rows. With lock_timeout=10s, you will get cancellations under load. 
GitHub

Recommendation

In sync, avoid updating operational columns (picked/is_picked/pick_status/locked_by_batch_id, etc.) if the row is locked or already picked.

If an invoice is actively being picked, either:

skip it and retry later, or

only update safe master-data fields that do not interfere with picking.

6) Reduce transaction “blast radius” for date imports

Your sync loops invoices and commits, which is good; however, the heaviest queries (DW prefetch, mass updates) can still be too large per invoice batch.

Recommendation

Keep transactions small (one invoice per transaction is good; consider one chunk of lines per transaction if needed).

Log timing per invoice (duration, rows updated, lock waits) so you know exactly where it slows down.

P2 — Important operational hardening
7) Stop doing schema creation/updates automatically at app startup in production

You call db.create_all() and run schema-update helpers on startup. That can introduce startup latency, unexpected locks, and hard-to-debug differences across environments. 
GitHub

Recommendation

Move schema changes to a migration workflow (Alembic) or a controlled admin command.

Production app startup should be fast and predictable.

8) Add “sync-run observability”

When an import fails, you need enough telemetry to identify whether it was:

lock timeout

statement timeout

upstream PS365 response not JSON

network failure

worker timeout/restart

Recommendation

Persist a sync_runs table: started_at, finished_at, params (date/invoice), counts, error_type, error_excerpt.

Log: PS365 request duration + response content-type when failing.

Include a request_id in every log line for correlation.

If you want the quickest production fix for “date import fails”

Do these three first:

Raise/override the production DB timeouts for the sync operation. 
GitHub

Add a global JSON error handler for /api/* so the real error returns to the browser as JSON. 
GitHub

Move date import to a background job (scheduler/queue). 
GitHub
+1

If you want, I can provide a concrete implementation plan (job table schema + endpoints + minimal UI polling) that fits your current Flask/SQLAlchemy structure and can be handed to Replit as “copy/paste instructions.”